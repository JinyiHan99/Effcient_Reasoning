import json, os, shutil, re, random, io, time, random
os.environ['TOKENIZERS_PARALLELISM'] = 'true'
import torch
from bottle import request
import bottle, threading, queue
from utils import tensor_to_bytes, bytes_to_tensor, make_bytes_list, bytes_list_to_list, bytes_to_text
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn as nn
import asyncio

def get_per_token_logps(model, input_ids, batch_size=None):
    # 如果没有指定批大小或批大小大于等于输入的批量，直接使用原始方法
    if batch_size is None or batch_size >= input_ids.shape[0]:
        logits = model(input_ids).logits  # (B, L, V)
        logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit
        input_ids = input_ids[:, 1:]  # (B, L-1), exclude the first input ID
        per_token_logps = []
        input_ids = input_ids.to(logits.device)  
        for logits_row, input_ids_row in zip(logits, input_ids):
            log_probs = logits_row.log_softmax(dim=-1)
            token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)
            per_token_logps.append(token_log_prob)
        return torch.stack(per_token_logps)

    # 分批处理
    total_batch_size = input_ids.shape[0]
    all_per_token_logps = []

    # 按批次处理数据
    for i in range(0, total_batch_size, batch_size):
        end_idx = min(i + batch_size, total_batch_size)
        batch_input_ids = input_ids[i:end_idx]

        # 处理当前批次
        batch_logits = model(batch_input_ids).logits  # (mini_batch, L, V)
        batch_logits = batch_logits[:, :-1, :]  # (mini_batch, L-1, V)
        batch_input_ids = batch_input_ids[:, 1:]  # (mini_batch, L-1)
        batch_input_ids = batch_input_ids.to(batch_logits.device)

        # 计算当前批次的 log 概率
        batch_per_token_logps = []
        for logits_row, input_ids_row in zip(batch_logits, batch_input_ids):
            log_probs = logits_row.log_softmax(dim=-1)
            token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)
            batch_per_token_logps.append(token_log_prob)

        # 收集结果
        all_per_token_logps.extend(batch_per_token_logps)
    # 合并所有批次的结果
    return torch.stack(all_per_token_logps)

from openai import OpenAI

prompt_template = '''
        ### Question:
        {Question}

        ### Ground Truth:
        {Ground_Truth}

        ### Answer:
        {Answer}
        '''
system_prompt = '''Now, I want to test an AI assistant's ability to answer questions.
Below is a question, a ground truth answer, and a final answer generated by the AI assistant, which is wrapped in \\boxed{}.
Please rate the AI assistant's final answer according to the ground truth answer.
If you think the answer is correct, your output is 1; otherwise, your output is -1.
Your output is just -1 or 1.'''

client = OpenAI(base_url="http://10.176.64.144:59824/v1/", api_key="dummy")

def llm_eval(question, std, ans):
    matches = re.findall(r'\\boxed\{(.*?)\}', ans)
    final_answer =  "\\boxed{" + matches[0] + "}" if matches else ans
    user_prompt = prompt_template.format(Question = question, Ground_Truth = std, Answer = final_answer)
    response = client.chat.completions.create(
        model=client.models.list().data[0].id,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}],
        temperature=0.2,
        )
    res = response.choices[0].message.content
    try: 
        reward = float(res)

    except:
        numbers = re.findall(r"[-+]?\d*\.\d+|\d+", res)
        if numbers:
        # 取最后一个数字并转换
            last_number = numbers[-1]
            reward = float(last_number)
        else:
        # 如果没有找到任何数字，返回0
            reward = 0.0
    finally:
    # 确保奖励在0-1之间
        reward = max(0.0, min(reward, 1.0))
    return reward

class RefServer:
    def __init__(self, model_path, host='0.0.0.0', port=59877):
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)
        self.model.eval()
        self.model.requires_grad_(False)
        self.raw_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.app = bottle.Bottle()
        self.host = host
        self.port = port
    
    def run_server(self): 
        @self.app.route('/upload', method='POST')
        def do_upload():
            dd = request.body.read()
            dd = bytes_list_to_list(dd)
            # if len(dd) not in (8,9): return b'tensor'
            # print("!!收到的长度:",len(dd))
            # data = {'base': json.loads(dd[0])} 
            temp_dict = json.loads(dd[0])
            data= {}
            data['base'] = temp_dict['plen']
            ##在这里评估答案的正确性以及记录这几个值的答案
            scores = []
            for ans in temp_dict['answers']:
                score = llm_eval(temp_dict['question'], temp_dict['std'], ans)
                scores.append(score)
            

            data['inputs'] = bytes_to_tensor(dd[1])
            # data['rewards'] = bytes_to_tensor(dd[2])
            data['format'] = bytes_to_tensor(dd[3])
            # data['acc'] = bytes_to_tensor(dd[4])
            data['acc'] = torch.tensor(scores, dtype=torch.float32)
            data['length'] = bytes_to_tensor(dd[5])
            data['rewards'] = data['acc'] + data['format'] + data['length']
            # data['answers'] = bytes_to_text([6])
            data['gen_logps'] = bytes_to_tensor(dd[-1])
            self.raw_queue.put(data)
            return b'tensor'

        @self.app.route('/get', method='GET')
        def do_get():
            if self.result_queue.empty(): return b'empty'
            return self.result_queue.get()

        asyncio.set_event_loop(asyncio.new_event_loop())
        bottle.run(self.app, host='0.0.0.0', port=59877, server='tornado')

    def start(self):
        threading.Thread(target=self.run_server, daemon=False).start()
    
        param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
        gpu_total = torch.cuda.get_device_properties(0).total_memory
        if param_size > gpu_total * 0.8:
            print('\nAuto patch model to use CPU offloading, only support Qwen2 series now...\n')
            from .patch_for_cpu_offload import patch_qwen2
            patch_qwen2(self.model)
        else:
            self.model.to('cuda')

        ## 在这里启动一个评估模型用来评估模型的生成答案

        while True:
            d = self.raw_queue.get()
            tic = time.time()
            prompt_length = d['base']['plen']
            data = [json.dumps(d['base']).encode(), d['inputs'], d['rewards'],
                d['format'], d['acc'],d['length']
            ]  # ref server、gen_logp
            if 'end' not in d['base']:
                with torch.inference_mode():
                    if d['inputs'].shape[1]<3000:
                        per_token_logps = get_per_token_logps(self.model, d['inputs'].to(self.model.device), d['inputs'].shape[0])
                    else:
                        per_token_logps = get_per_token_logps(self.model, d['inputs'].to(self.model.device), 2 )
                per_token_logps = per_token_logps[:,prompt_length-1:]
                data.append(per_token_logps) 
            else: data.append(torch.tensor([0]))
            if 'gen_logps' in d: data.append(d['gen_logps'])
            data = [data[0]] + [tensor_to_bytes(t) for t in data[1:]]
            # print(f"data的长度：{len(data)}")
            xdata = make_bytes_list(data)
            self.result_queue.put(xdata)
            print('batch', len(data), d['base']['plen'], d['inputs'].shape, d['rewards'], f' time: {time.time() - tic:.2f}s')
            if random.random() < 0.1: print(f'raw_queue: {self.raw_queue.qsize()}, result_queue: {self.result_queue.qsize()}')






if __name__ == '__main__':
    # RefServer(model_path='/data2/Qwen/Qwen2.5-7B-Instruct').start()
    RefServer(model_path="/data2/Qwen/DeepSeek-R1-Distill-Qwen-7B").start()
